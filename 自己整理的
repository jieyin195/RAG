看视频，通俗易懂的了解什么是RAG
暂时无法在HONOR E Link文档外展示此内容
前言
RAG（Retrieval-Augmented Generation）检索增强生成，即大模型LLM在回答问题或生成文本时，会先从大量的文档中检索出相关信息，然后基于这些检索出的信息进行回答或生成文本，从而可以提高回答的质量，而不是任由LLM来发挥。
RAG的工作原理
RAG的工作原理可以分为两个主要部分：
1. 信息检索（Retrieval）： 首先，RAG会从一个大型数据库或知识库中检索出与输入内容相关的信息。这一步骤确保了生成的内容具有高相关性和准确性。
2. 生成（Generation）： 接下来，RAG会基于检索到的信息生成新的文本内容。这一步骤利用了先进的生成模型，如GPT-4，来创建流畅且有意义的文本。
RAG的优势
- 高效性： RAG能够在极短的时间内完成信息检索和文本生成，大大提升了工作效率。
- 高质量： 由于结合了检索和生成技术，RAG生成的内容不仅相关性高，而且质量优异。
- 广泛应用： RAG可以应用于多个领域，如写作、数据分析、客户服务等，几乎涵盖了所有需要文本生成的场景。
RAG 适用场景：
- 第一：私有数据存在一定频率的动态更新的；
- 第二：需要给出引用原文的；
- 第三：硬件资源（GPU）不是太充足的（即使用RAG也需要微调，但一次微调处处可用，远比每个企业私有库微调一个模型成本低的多）
RAG任务几个具体问题场景【RAG行业交流中发现的一些问题和改进方法】：
1. 不擅长小范围的描述性问题回答。例如，哪个主体具有某些特征?
2. 不擅长关系推理。即寻找从实体A到实体B的路径或识别实体集团。
3. 不擅长时间跨度很长的总结。例如，“列出所有哈利波特的战斗”或“哈利波特有多少次战斗?”
RAG就像刚刚诞生的婴儿，你需要一步一步教它。
你给RAG一段话：昨天是今天的前一天
你问今天是什么？它肯定答不出来，但你要是在上面那段话加上“昨天是今天的前一天，今天是美好的一天。”那么今天就是美好的一天，但是，如果你问：“昨天是什么？”那么答案有可能就是昨天是美好的一天的前一天。
[图片]

[图片]
好了，RAG就讲到这里，具体详细的可以参阅网上资料，篇幅有限，就不过多赘述了。
[图片]
我们要解决的就是上图RAG核心技术层的第一步：提取。
首先要知道什么是结构型数据和非结构型数据，
1. 结构型数据：这种数据通常是高度组织化的，具有固定的格式和结构，便于存储和检索。常见的结构型数据包括关系数据库中的表格数据、电子表格等。例如，SQL数据库中的数据就是典型的结构型数据。
2. 非结构型数据：这种数据没有固定的格式或结构，通常是文本、图像、音频、视频等形式。非结构型数据的处理和分析通常比结构型数据更复杂。常见的非结构型数据包括电子邮件、社交媒体帖子、文档、图片和视频等。
结构型数据这里直接使用数据库同步技术，我们重点是讲解非结构型数据。
以文件举例，那么拿到一个文件后，我们应该怎么把内容提取出来入库呢？整体流程如下所示
[图片]
我们要做的其实只有上图步骤的1,2,3,4。剩下的就交给LLM了。
知道了整体流程，再来看看应用架构：
[图片]
1 RAG的挑战
1.1 文档种类多
文档格式多样，包括docx、doc、pptx、ppt、excel、pdf、rar、zip等，其中pdf还分为扫描版和文字版，压缩包里面还可以套娃。压缩包套娃看上去复杂，处理过程并不是很复杂。但是处理ppt和pdf文件就显得尤为困难，因为ppt中包含大量的架构图、流程图等图示以及展示图片，pdf文件也有类似情况。提取出来的文字信息往往是碎片化和不完整的。
PPT的难点在于如何提取其中大量的流程图和架构图。这些图多以形状元素的形式存在于PPT中，如果仅仅提取文字，大量潜在的信息将会完全丢失。
另一个问题是如何还原图片信息。许多文档采用图文混排的形式，例如上述的PPT文件，转换成PDF后，仅能识别出某块内容是一幅图片。对于这些图片，直接转换成向量不利于后续的检索。
1.2 文档嵌套
文档里面嵌套其他的文件是常有的事，比如PDMC里面的文件，任职报告等。在一个地方嵌套一个文件，还想要上下文统一是很难的事情，如果是压缩包，可能这个地方只需包里文件的标题，而不是内容。
1.3 不同文档结构影响，需要不同的切片方式
根据文档结构特性使用不同的切片方式，不好的切片方式会造成：
- 如果切片太大，查询精准度会低
- 一段完整的话可能被切成好几块，每一段文本包含的语义信息实际上也是不够完整的
- 一些鸡肋切片，其实可以删掉
1.4 用户提问的随意性 + 大众对RAG的定位混乱
大部分用户在提问时，写下的query是较为模糊笼统的，其实际的意图埋藏在了心里，而没有完整体现在query中。使得检索出来的文本段落并不能完全命中用户想要的内容，大模型根据这些文本段落也不能输出合适的答案。
同时，企业员工在使用RAG时，所发起的查询需求都是哪些，企业对RAG的定义也许是【信息抽取问答+数值计算问答+逻辑推理类问答+...】的一个“全家桶”，“狭义RAG方案”在“广义RAG需求”上往往是不匹配的，查询效果也达不到预期。
1.5 全文/多文类意图失效
来自：瀚海方舟：ChatGPT应用：如何征服市场眼中的“万能RAG”
提问：近期《独家新闻》系列文章对哪些行业关注度最高？
受限于文档切割，遇到横跨多篇文章，或全篇文章的提问，基本上凉凉了 。
1.6 多条件约束失效
来自：瀚海方舟：ChatGPT应用：如何征服市场眼中的“万能RAG”
提问：昨天《独家新闻》统计的化学制品行业的关注度排名第几？
加上约束之后，如何让大模型读懂什么叫“昨天”，又有哪段内容属于《独家新闻》？
2 知识库文档预处理
在载入知识库文件的时候，直接上传文档虽然能实现基础的问答，但是，其效果并不能发挥到最佳水平。因此，我们对知识库文件做出以下的预处理。 以下方式的预处理如果执行了，有概率提升模型的召回率。
2.1 使用TXT / Markdown 等格式化文件，并按照要点排版
修改后的Markdown文件，具有更高的召回率（推荐的模型组合）
在原有的文件上面自定义一些规律，比如PPT我们自己加了很多“Slide#序号”，用来区分PPT的哪一页等。
2.2 减少文件中冲突的内容，分门别类存放数据（工作量较大，暂时未做）
就像人类寻找相关点一样，如果在多份文件中存在相似的内容，可能会导致模型无法准确的搜索到相关内容。 因此，需要减少文件中相似的内容，或将其分在不同的知识库中。 例如，以下两个句子中，如果搜索外籍教师，则具有歧义，非常容易搜索到错误答案。
文件一：
在大数据专业中，我们已经拥有超过1/3的外籍博士和教师。

文件二：

本专业具有40%的外籍教师比例，
本专业有博士生10人，研究生12人。
2.3 减少具有歧义的句子
知识库中应该减少具有歧义的句子和段落，或者汉语的高级用法，例如
他说他会杀了那个人。
你说啥子？
我喜欢你的头发。
地板真的滑，我差点没摔倒。
在相似度模型对比的时候，仅仅能搜索句子的表面意思，因此，使用有歧义的句子和段落可能导致搜索错误。
2.4 结构复杂的先根据大模型以问答对的形式输出
一些报告图表较多的，可以先让大模型理解后，生成一些问答对；
允许的话，人工简单浏览一下，比如这篇提到的：一意AI增效家：1万页PDF转大模型数据集2！批量把txt文本拆成CSV！准备生成问答对！27/45
2.5 对文档合理分块
不合理的分块会导致很多问题，过小的块导致上下文信息的缺失。
2.6 数据清洗
没接触大模型会觉得，大模型无所不能，啥数据，啥格式都可以接纳，但其实还是要走之后数据挖掘的老路，数据清洗就是一条老路，但是不得不走。
如果您的数据是从web爬取的，您可能需要删除HTML标记或特定的元素，保证文本的“纯洁”，减少文本的噪音。
3 基于LLM的函数插件(function_call)
3.1 function_call
函数调用（Function Calling）是 OpenAI 在 6 月 13 日发布的新能力。根据官方博客描述，函数调用能力可以让模型输出一个请求调用函数的消息，其中包含所需调用的函数信息、以及调用函数时所携带的参数信息。这是一种将 GPT 能力与外部工具/API 连接起来的新方式。
通常大模型一般只有自己训练的相关数据，没有获取当前时间、外界知识的能力。但是通过function_calling我们可以将一些系统功能通过插件的方式，供大模型调用。使用示例：
1. 获取当前时间
[图片]
2. 获取实时咨询
[图片]
3. 绘图
[图片]
3.2 function_call的基本使用
一个函数由name、description、parameters参数组成。GPT根据function的表述和各种参数的描述，生成方法的调用和参数组成的JSON，从而可以更方便的调用外部系统API。
创建日程的function_call示例如下:
        "functions":[
            {
                "name": "create_calendar",
                "description": "识别用户是否想要创建日程或者会议。如果是的话，则根据提供的时间、地点、主题创建日程或者会议,例如 1.立刻开一个会议 2.预约周四下午的会议3.预约下周三上午的会议。此时你需要获取当前时间，然后推演需要创建日程的时间并预约日程",
                "parameters": {
                    "type": "object",
                    "properties": 
                        {
                            "location":{
                                "type":"string",
                                "description":"地点,例如 3405会议室"
                            },
                            "time":{
                                "type":"string",
                                "description":"时间格式为: yyyy-MM-dd HH:mm,例如: 2024-01-16 09:39"
                        
                            },
                            "subject":{
                                "type":"string",
                                "description":"会议主题,例如 概要设计评审"
                            }
                        }
                
                }
            },
            {
                "name": "get_current_time",
                "description": "获取当前年月日，以及时间。用于需要时间的场景",
                "parameters": {
                    "type": "object",
                    "properties": {
                        
                    }
                }
            }
    ]
           输入：帮我创建2024年7月15日上午9点的日程，主题是团建，地点在安托山
  输出：生成相应API参数
"message": {
    "role": "assistant",
    "content": null,
    "function_call": {
        "name": "create_calendar",
        "arguments": "{\"location\":\"安托山\",\"subject\":\"团建\",\"time\":\"2024-07-15 09:00\"}"
    }
}
functions的编排:
当同时传入多个function到大模型时，大模型可以分析提问的内容，调用各种可以使用的函数并进行编排，以完成任务，以创建日程为例:
暂时无法在HONOR E Link文档外展示此内容
4 智小荣RAG实践
4.1  智小荣内部知识问答流程
[图片]

4.1.1 知识解析上传
1. 业务人员上传公司私域知识 (PDF、DOC、TXT等)
2. 文件解析为Markdown
3. 文本拆分
  合理的分块策略可以确保搜索结果准确地捕获用户查询的本质。如果我们的块太小或太大，可能会导致不精确的搜索结果或错过展示相关内容的机会
  - 固定文本拆分
    固定大小的重叠滑动窗口方法是一种简单的文本分块方法，它将文本分成固定大小的块。微软建议分块大小为512
[图片]
    拆分的过程中，上下分块之间应当保留一定的overlap，保证上下文语义连贯性
[图片]
  - 段落拆分
  段落拆分考虑了文本的固有结构。这种方法不使用固定大小的窗口，而是根据文本的自然划分(如句子、段落、节或章节)将文本划分为块。
[图片]
  这种方法尊重文本的自然语言边界，确保单词、句子和思想不会在中间被打断。这有助于保持每个块内信息的语义完整性。但是如果导入的知识过于杂乱，并且没有固定的段落结构和标识符，需要对源知识做一定的清洗。
  - 主题拆分
    将知识进行清洗后，按照内容层次进行拆分。这种拆分和段落拆分方法类似，最大保留了内容的结构关系和语义的完整性，但是需要进行复杂的数据清洗工作
[图片]
[图片]
  - 表拆分
    在员工费用报销的实践中，以上方案可能在案例库中都得不到较好的向量索引性能。针对案例库的表格，可以选择按行拆分，保证每一个分块表达的完整的语义。
暂时无法在HONOR E Link文档外展示此内容
在不同的场景可能需要选择不同的拆分方法，目前智小荣选择了固定拆分、句拆分、表拆分的结合方案。
4.1.2 向量转换
语义向量模型（Embedding Model）被广泛应用于搜索、推荐、数据挖掘等重要领域，将自然形式的数据样本（如语言、代码、图片、音视频）转化为向量（即连续的数字序列），并用向量间的“距离”衡量数据样本之间的“相关性” 。
  
[图片]
embedding转化示例：
[图片]
为什么需要向量转换？
提到检索，我们通常会使用elastic search，但是es是基于关键字的检索，有其局限性，同一个语义，用词不同，就会导致无法检索出想要的数据
举例：
es中存储 “小明喜欢吃汉堡”，如果检索关键字“小明喜欢吃”，那么可以找到“汉堡”，但是如果检索“小明爱吃”，那么就找不到这个数据
向量化模型的选型:
各个开源模型的对比：
暂时无法在HONOR E Link文档外展示此内容
[图片]
混合检索的支持：
BGE_M3模型支持生成两种维度向量，不同维度向量分别适合不同场景的搜索：
  Dense： 密集检索，使用离散表示，多用于语义检索。
  Sparse:  稀疏检索，使用连续的向量空间，多用于关键字检索。
[图片]
4.1.3 向量检索
  向量检索基本流程
[图片]
  1. 知识文档拆分
  2. 知识文档向量化入库
  3. 搜索词向量化
  4. 向量检索相关度前K条分片
  5. 将搜索知识发送给大模型
  重排序流程
  
[图片]
   
    原始做法：通过向量库搜索到前K个相似的文本段，直接提交给大模型进行分析、回答。
    缺点：
    1. 返回的文本段不一定相似度最高。
    2. K值过高，则给大模型输入太多，会影响回答准确性和回答速度。
    3. K值过低，则可能搜索不到想要的文本段
    改进做法：通过向量库搜索到前K个相似的文本段，再依次计算前K个文本段和提问的向量相似度，提取前N个分数最高的文本段，再提交给大模型进行分析、回答。
    加入重排序后，数据量对搜索精度的影响：
[图片]
  重排序示例：
[图片]
4.2 内部知识问答优化
- 重排序过滤
暂时无法在HONOR E Link文档外展示此内容
通过embedding的分数来对知识进行过滤，当知识库中没有用户相关知识时，较大概率引用不相关知识，从而干扰大模型的回答：
[图片]

[图片]

[图片]
- 问题拆分
1.  将问题通过LLM拆分为多个子问题，分别进行检索。
2. 将子问题的检索答案和原问题进行整合，送入大模型
[图片]

流程图示意：
[图片]
- 搜索路由分发
[图片]
4.3 RAG常见问题
[图片]
RAG常见的7种问题：
1.Missing Content
  无法从可用文档中回答的问题。在满意的情况下，RAG 系统会以“对不起，我不知道”之类的方式响应。但是，对于与内容相关但没有答案的问题，系统可能会被愚弄给出答案
2.Missed the Top Ranked Documents
  问题的答案在文档中，但排名不够高，无法返回给用户。从理论上讲，所有文档都会在后续步骤中进行排名和使用。但是，在实践中，返回排名靠前的 K 个文档，其中 K 是根据性能选择的值
3.Not in Context 
  从数据库中检索了带有答案的文档，但未将其放入生成答案的上下文中。当从数据库返回许多文档并执行合并过程以检索答案时，就会发生这种情况 不涉及
4.Not Extracted
  在这里，答案存在于上下文中，但大型语言模型未能提取出正确的答案。通常，当上下文中有太多噪音或相互矛盾的信息时，就会发生这种情况
5.Wrong Format
  该问题涉及以某种格式（如表格或列表）提取信息，而大型语言模型忽略了该指令。 不涉及
6. Incorrect Specificity
  答案在响应中返回，但不够具体或过于具体，无法满足用户的需求。当 RAG 系统设计人员对给定问题（例如教师对学生）具有所需的结果时，就会发生这种情况。在这种情况下，应该提供特定的教育内容，而不仅仅是答案。  当用户不确定如何提出问题并且过于笼统时，也会发生不正确的特异性
7.Incomplete
  不完整的答案不是不正确的，但会遗漏一些信息，即使这些信息在上下文中并且可以提取。示例问题，例如“文档 A、B 和 C 涵盖哪些要点？更好的方法是分别提出这些问题
5 私有化模型部署实践
5.1 LM Studio
LM Studio是一款桌面应用程序，可在您的计算机上运行本地大型语言模型（LLMs）。
下载地址:https://lmstudio.ai/
GGUF模型下载：https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/tree/main
[图片]
优点：有可视化界面、可随时修改参数、下载模型方便
缺点：无法进行多线程问答，仅适合个人爱好者单机部署，不适合企业级部署
5.2 Llama
1. 下载gguf模型文件(与LM Studio一致)
2. linux安装Llama
  上传llama.cpp 压缩文件并解压
  cd 进llama.cpp目录，执行
  make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc
暂时无法在HONOR E Link文档外展示此内容

3. 执行命令
./server  -m /zxrdisk/llm/model/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf     -c 2048 -np 10 -ngl 10000   --top-p 0.95 --host 0.0.0.0 --port 8080 
优点：可多线程问答，部署方便
缺点：GPU利用率较低，回答速度慢
5.3 vllm
1. 下载safetensors模型文件
下载地址：https://huggingface.co/Qwen/Qwen2-72B-Instruct/tree/main
2. 安装 vllm
pip install vllm
3. 部署
  命令： 
  python3 -m vllm.entrypoints.openai.api_server --model /zxrdisk/model/QwenQwen2-72B-Instruct  --dtype auto --host 0.0.0.0 --port 8080 --max-model-len 16000  --tensor-parallel-size 4 --gpu-memory-utilization 0.90
优点：可多线程问答、部署方便、GPU使用率高、回答速度快
缺点：仅支持safetensors模型文件部署，对显存要求较高
5.4 速度对比测试
llama.cpp 部署Qwen1.5_110B,推理时GPU使用率：
[图片]
vllm 部署Qwen1.5_110B,推理时GPU使用率：
[图片]
速度对比：
推理框架
model
GPU
prompt长度
第一个token出现时间
完成时间
llama.cpp
Qwen1.5_110B
8*24G
1179
5s
12.19s
vllm
Qwen1.5_110B
8*24G
1179
1s
1.7s
压测结果:
llama.cpp -  Qwen1.5_110B - 8*24G:
  吞吐量   ：2.9 QPM
[图片]
vllm -  Qwen1.5_110B -8*24G:
         吞吐量  : 72 QPM
[图片]
6 总结
RAG（检索增强生成）是一种先进的技术，它结合了信息检索和文本生成能力，旨在提高生成内容的相关性和质量。该技术分为两个核心步骤：首先从大型知识库中检索相关信息，然后基于这些信息生成准确且流畅的文本。RAG的优势在于其高效性、高质量的输出以及广泛的应用场景，适用于从客户服务到数据分析等多个领域。
RAG面临的挑战主要包括处理多样化的文档格式（如PDF、PPT的复杂性）、文档嵌套问题、切片策略对查询精准度的影响、用户提问的模糊性、以及对全文或跨文档查询的支持不足。为了优化RAG的性能，文档预处理至关重要，包括转换为更易处理的格式（如TXT或Markdown）、减少文件内的冲突和歧义、对结构复杂文档的问答化处理、合理分块以及严格的数据清洗。这些步骤能显著提升模型的召回率和查询准确性，确保RAG更好地满足用户需求，尤其是在企业知识管理和客户服务的场景中。
